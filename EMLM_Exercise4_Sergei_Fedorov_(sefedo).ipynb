{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sergei-N-Fedorov/Data_Analysis/blob/main/EMLM_Exercise4_Sergei_Fedorov_(sefedo).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a597809e",
      "metadata": {
        "id": "a597809e"
      },
      "source": [
        "# Exercise 4 | TKO_7092 Evaluation of Machine Learning Methods\n",
        "---\n",
        "\n",
        "Student name: **Sergei Fedorov**<br>\n",
        "Student number: **2511405**<br>\n",
        "Student email: sefedo@utu.fi<br>\n",
        "\n",
        "---\n",
        "\n",
        "The deadline for returning this exercise is **25.2.2026**.\n",
        "\n",
        "If you have any questions about this exercise, please contact Riikka Numminen (rimanu@utu.fi) in good time before the deadline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe7c871",
      "metadata": {
        "id": "dfe7c871"
      },
      "source": [
        "## Nested cross-validation for feature selection\n",
        "In this exercise, the task is to use **leave-one-out cross-validation** for model selection to understand the effect of the winner's curse.\n",
        "This is demonstrated by using **greedy forward selection** and a random binary data set.\n",
        "The data set is a balanced sample of size 60 (i.e. 30 positives and 30 negatives) with a hundred features. The data are i.i.d., and every feature follows a Bernoulli distribution with $p=0.5$. Thus, there is no signal in the data.\n",
        "\n",
        "The model to be used is **1-nearest neighbour** with **10 features**, and the greedy forward selection is used to select the best 10 features among all the features.\n",
        "Leave-one-out cross-validation is used for performance evaluation, and the prediction performance is measured as **accuracy**.\n",
        "\n",
        "### Greedy forward feature selection\n",
        "Greedy forward feature selection is an iterative feature selection process, where the features are selected one by one, avoiding a need to iterate through every possible combination of features. The features are selected as follows:\n",
        "- First, every feature is tested solely and the best is selected.\n",
        "- Then the selected feature is tested together with any other remaining feature and the best such a set of two features is selected.\n",
        "- Then that set of the selected two features is tested together with any other remaining feature and the best set of three features is selected.\n",
        "- The process is then continued accordingly until the desired amount of features is selected.\n",
        "\n",
        "### Implement the following tasks to complete this exercise:\n",
        "1. Use leave-one-out cross-validation to select the best 10 features. Report the optimal set of features and the corresponding accuracy.\n",
        "2. Use nested leave-one-out cross-validation (leave-one-out on both layers of cross-validation) to obtain an estimate of the prediction accuracy on unseen data, when the final hypotheses are obtained according to the procedure in the first step.\n",
        "3. Explain the difference in the obtained accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "050c9b00",
      "metadata": {
        "id": "050c9b00"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1df6b9e8",
      "metadata": {
        "id": "1df6b9e8"
      },
      "outputs": [],
      "source": [
        "# Import the libraries needed\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a15d9b4",
      "metadata": {
        "id": "4a15d9b4"
      },
      "source": [
        "### Load the data\n",
        "The labels are saved in a file *y_generated.csv*, and the features in file *X_generated.csv*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "01c06938",
      "metadata": {
        "id": "01c06938",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6104c620-74b8-43da-c090-2af553208f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60, 100)\n",
            "(60, 1)\n"
          ]
        }
      ],
      "source": [
        "# Read the data files. Verify the data dimensions\n",
        "X = pd.read_csv('X_generated.csv', header = None, index_col = False)\n",
        "y = pd.read_csv('y_generated.csv', header = None, index_col = False)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loaded dataset has 60 rows (datapoint) and 100 columns (features)."
      ],
      "metadata": {
        "id": "3_I2kTeXL4Hf"
      },
      "id": "3_I2kTeXL4Hf"
    },
    {
      "cell_type": "markdown",
      "id": "602dda34",
      "metadata": {
        "id": "602dda34"
      },
      "source": [
        "### Leave-one-out cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for the greedy forward feature selection\n",
        "\n",
        "def greedy_forward(X_input, y_input, selected):\n",
        "  '''\n",
        "  Function that returns the next best feature in addition to `selected`\n",
        "  and the accuracy of the model with the extended feature subset.\n",
        "  Selecting is managed by the cross-validation method `cv`.\n",
        "  '''\n",
        "  n_features = X_input.shape[1]   # total number of features (100)\n",
        "  acc = np.zeros(n_features)      # accuracies of the model along all the data\n",
        "                                  # when each feature is tried as\n",
        "                                  # complementary to the given subset `selected`\n",
        "  for f in range(n_features):\n",
        "    if f not in selected:             # choose one new feature,\n",
        "      subset = selected + [f]         # add it to the subset,\n",
        "      X_current = X_input[:, subset]  # and crop the data accordingly\n",
        "      y_pred = np.zeros(len(y_input))\n",
        "\n",
        "      loo = LeaveOneOut()             # LOOCV splits for the input data\n",
        "      for train_idx, test_idx in loo.split(X_current):\n",
        "        X_train, X_test = X_current[train_idx], X_current[test_idx]\n",
        "        y_train, y_test = y_input[train_idx], y_input[test_idx]\n",
        "\n",
        "        knn = KNeighborsClassifier(n_neighbors=1)\n",
        "        knn.fit(X_train, y_train)\n",
        "        y_pred[test_idx] = knn.predict(X_test)\n",
        "\n",
        "      acc[f] = accuracy_score(y_input, y_pred)\n",
        "\n",
        "  #with np.printoptions(precision=2):\n",
        "  #  print(acc)\n",
        "\n",
        "  best = int(np.argmax(acc)) # index of the feature that yielded the best score\n",
        "  # print(f\"Best accuracy when adding {len(selected) + 1} feature (namely {best}): {acc[best]:.2f}\")\n",
        "  return best, acc[best]"
      ],
      "metadata": {
        "id": "JZtxAZOnkgbv"
      },
      "id": "JZtxAZOnkgbv",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X.columns  # list of integers\n",
        "X = X.values          # store the data as numpy arrays\n",
        "y = y.values.ravel()"
      ],
      "metadata": {
        "id": "n_s45f6YF44p"
      },
      "id": "n_s45f6YF44p",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cf29aa64",
      "metadata": {
        "id": "cf29aa64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c696024c-5db7-4694-9b28-9d9a0a26dd59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 0.8333\n"
          ]
        }
      ],
      "source": [
        "# Greedy forward feature selection via regular LOOCV\n",
        "\n",
        "selected = []       # features (indexes) selected by the method\n",
        "acc_10 = 0          # resulting accuracy for the selected 10-feature subset\n",
        "for i in range(10):\n",
        "  best = greedy_forward(X, y, selected)\n",
        "  selected.append(best[0])\n",
        "  if i == 9:\n",
        "    acc_10 = best[1]\n",
        "\n",
        "print(f\"\\nSelected subset of the features: {selected}\")\n",
        "print(f\"Accuracy for this subset: {acc_10:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa7e3de",
      "metadata": {
        "id": "5aa7e3de"
      },
      "source": [
        "### Nested leave-one-out cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "81fe5adf",
      "metadata": {
        "id": "81fe5adf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db56aaa-f599-434f-f88c-6416f3ac9867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Outer split number: 1\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 2\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 9, 3]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 3\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 55, 36, 26, 16, 17]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 4\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 76, 51, 55, 83]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 5\n",
            "Selected subset of the features: [0, 1, 57, 33, 18, 72, 65, 70, 4, 61]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 6\n",
            "Selected subset of the features: [0, 1, 94, 26, 75, 35, 6, 11, 64, 28]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 7\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 24, 10, 72]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 8\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 24, 31, 16]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 9\n",
            "Selected subset of the features: [0, 1, 94, 39, 87, 4, 72, 97, 51, 21]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 10\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 76, 51, 5, 29]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 11\n",
            "Selected subset of the features: [0, 32, 1, 98, 81, 49, 47, 29, 11, 6]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 12\n",
            "Selected subset of the features: [0, 1, 94, 66, 45, 48, 19, 26, 70, 13]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 13\n",
            "Selected subset of the features: [0, 1, 54, 4, 66, 72, 34, 59, 46, 12]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 14\n",
            "Selected subset of the features: [0, 1, 12, 21, 39, 2, 4, 10, 95, 5]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 15\n",
            "Selected subset of the features: [0, 1, 94, 38, 64, 21, 4, 86, 52, 71]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 16\n",
            "Selected subset of the features: [0, 1, 94, 66, 45, 48, 19, 26, 68, 74]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 17\n",
            "Selected subset of the features: [0, 1, 48, 18, 4, 75, 86, 10, 53, 27]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 18\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 19\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 20\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 55, 36, 4, 78, 76]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 21\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 22\n",
            "Selected subset of the features: [0, 1, 75, 20, 31, 6, 28, 74, 61, 4]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 23\n",
            "Selected subset of the features: [0, 1, 54, 4, 22, 57, 85, 50, 92, 16]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 24\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 25\n",
            "Selected subset of the features: [0, 1, 94, 66, 45, 48, 19, 26, 68, 70]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 26\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 24, 31, 16]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 27\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 24, 55, 35]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 28\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 71, 35, 27, 64, 66]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 29\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 31, 39, 3]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 30\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 24, 31, 16]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 31\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 35, 87, 27, 32]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 32\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 35, 38, 26, 27]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 33\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 21, 11, 39, 75]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 34\n",
            "Selected subset of the features: [0, 1, 94, 66, 45, 12, 35, 19, 32, 52]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 35\n",
            "Selected subset of the features: [0, 1, 94, 64, 60, 73, 77, 8, 5, 62]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 36\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 37\n",
            "Selected subset of the features: [0, 1, 94, 66, 45, 35, 12, 19, 26, 75]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 38\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 4, 38, 24, 41]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 39\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 76, 51, 5, 73]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 40\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 24, 31, 41]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 41\n",
            "Selected subset of the features: [0, 1, 94, 66, 45, 82, 4, 12, 52, 54]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 42\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 76, 11, 21, 39]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 43\n",
            "Selected subset of the features: [0, 1, 89, 93, 22, 4, 59, 17, 24, 45]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 44\n",
            "Selected subset of the features: [0, 1, 94, 66, 45, 48, 19, 26, 28, 36]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 45\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 46\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 47\n",
            "Selected subset of the features: [0, 1, 94, 39, 87, 4, 72, 51, 97, 21]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 48\n",
            "Selected subset of the features: [0, 1, 94, 66, 45, 35, 12, 19, 26, 75]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 49\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 50\n",
            "Selected subset of the features: [0, 1, 54, 4, 66, 34, 72, 48, 74, 75]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 51\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 55, 36, 4, 78, 41]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 52\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 9, 56]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 53\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 54\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 55\n",
            "Selected subset of the features: [0, 1, 54, 4, 66, 34, 72, 59, 18, 17]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 56\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 4, 38, 24, 41]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 57\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 1.0000\n",
            "\n",
            "Outer split number: 58\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 73, 55, 24, 35]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 59\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 64, 35, 87, 27, 32]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Outer split number: 60\n",
            "Selected subset of the features: [0, 1, 94, 65, 6, 55, 41, 33, 36, 39]\n",
            "Accuracy for this subset: 0.0000\n",
            "\n",
            "Average accuracy over outer splits: 0.5000\n"
          ]
        }
      ],
      "source": [
        "# Nested LOOCV for feature selection\n",
        "\n",
        "resulting_accuracies = []  # accuracy values for the models over outer splits\n",
        "split_count = 0\n",
        "\n",
        "cv = LeaveOneOut()\n",
        "for train_idx, test_idx in cv.split(X):       # outer split\n",
        "  X_train, X_test = X[train_idx], X[test_idx]\n",
        "  y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "  fold_selected = []    # the current feature subset selected on the outer fold\n",
        "  for i in range(10):\n",
        "    best = greedy_forward(X_train, y_train, fold_selected)  # inner CV inside\n",
        "    fold_selected.append(best[0])\n",
        "\n",
        "  model = KNeighborsClassifier(n_neighbors=1)\n",
        "  model.fit(X_train[:, fold_selected], y_train)    # fitting the model with\n",
        "                               # the selected features on the training data\n",
        "  y_pred = model.predict(X_test[:, fold_selected]) # predicting on unseen data\n",
        "  fold_acc = accuracy_score(y_test, y_pred)        # 0 / 1 (on one test point)\n",
        "  resulting_accuracies.append(fold_acc)\n",
        "\n",
        "  split_count += 1\n",
        "  print(f\"\\nOuter split number: {split_count}\")\n",
        "  print(f\"Selected subset of the features: {fold_selected}\")\n",
        "  print(f\"Accuracy for this subset: {fold_acc:.4f}\")\n",
        "\n",
        "print(f\"\\nAverage accuracy over outer splits: {np.mean(resulting_accuracies):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Note:_  In the output above, the accuracy for each subset and, hence, each outer split is calculated on a single test datapoint. It's, therefore, 0 if the prediction was wrong, and 1 if it was correct.\n",
        "The resulting accuracy is then the average value of them."
      ],
      "metadata": {
        "id": "JwmpRHUoRg2E"
      },
      "id": "JwmpRHUoRg2E"
    },
    {
      "cell_type": "markdown",
      "id": "c25eebac",
      "metadata": {
        "id": "c25eebac"
      },
      "source": [
        "### Analysis of the results\n",
        "\n",
        "In the case of the plane LOOCV, we select an optimal feature subset and, at the same time, estimate the model performance quality on the same data. This means the model has been adjusted to the dataset before testing. More exactly, we chose the model that, out of many other models, accidentaly showed a good result on this dataset which was not because of the high quality of this model or of such models on average. Therefore, the estimation is too optimistic.\n",
        "\n",
        "On the opposite, the nested LOOCV leaves a test datapoint safe and makes possible to evaluate models on the truely unseen data. It doesn't build the estimation on one (maybe lucky) model but consider multiple models constructed with one method. So, it estimates behaviour of such models in general. Due to this fact, we get more fair assessment of the quality of the feature selection method for 1NN classifier.\n",
        "\n",
        "The resulting accuracy value of 0.5, provided by Nested CV, reflects the real situation on this random dataset: no model is able to predict well the labels from the data with no signal. The initial estimation (0.83) obtained from LOOCV is obviously not relevant in this context."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "984f5e1c",
      "metadata": {
        "id": "984f5e1c"
      },
      "source": [
        "### AI usage"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI was used to explain me\n",
        "\n",
        "- the key idea of using the nested CV: what its results exactly tell us (that this is not about the model itself but rather the method of building this model),\n",
        "\n",
        "- some technical stuff about numpy, in particular computational efficiency,\n",
        "\n",
        "- reasons of errors in the code."
      ],
      "metadata": {
        "id": "56Mn3U_vm6-O"
      },
      "id": "56Mn3U_vm6-O"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}